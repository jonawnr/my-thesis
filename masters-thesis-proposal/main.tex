% REMOVE THIS !TEX TS-program = pdflatexmk
\documentclass[sigplan, nonacm]{acmart}

\title{Detecting Edited Knowledge in Black-Box PLMs by asking Unrelated Questions}
\author{Jonathan Wagner}

\begin{document}
    \maketitle
    
    \section{Introduction}
   	\begin{itemize}
    		\item Importance of LLMs
    		\item What is Knowledge Editing? \cite{youssef_detecting_2024}
		\item What harm can it do?
    		\item Why do we want to detect edited knowledge?
    		\item Why do we want to use the black-box approach?
    	\end{itemize}
        
    \section{Related Work}
    	What have others done to solve a similar problem or answer the same or a similar question?    
    \section{Research Questions}
   	\begin{itemize}
		\item Is it possible to use this approach for edited knowledge?
		\item What is the ideal model for it?
		\item What is the performance?
		\item Advantages and disadvantages compared to other methods
	\end{itemize}
    
    \section{Approach}
        \begin{itemize}
		\item Identify suitable data sets, LLMs, KE techniques
            \begin{itemize}
    		\item Data sets -> ZSRE and CounterFact. Why?
    		\item LLMS -> GPT-2-XL and GPT-J. Why?
                \item KE -> ROME, MEMIT. Why?
	    \end{itemize}
		\item Identify prompt framework
            \item -> For the experimental setup, use framework from the paper, later maybe adjust?
		\item Identify experimental setup
            \item -> Editing GPT-2-XL, because it is the smaller model with ROME, because it is faster and CounterFact/ZSRE (why)? Similar to the paper edit 1000 facts use 324 for training and the rest for test (why?)
		\item Identify methods to measure performance
            \item -> Check paper for editing and classifier performance
	\end{itemize}

    \section{Preliminary Results}
    	Running first tests and describing them
    
    \section{Time Plan}
    	Time Plan. Can be found online and adapted to the topic
    
    \section{GenAI Disclosure}
        *How did I use GenAI, and describe how it did help or not help*
        The proposal was developed with the assistance of AI-based tools, including ChatGPT, DeepSeek, and TabNine. TabNine, integrated into Visual Studio Code, was utilized primarily for advanced auto-completion, significantly expediting the process of writing code. ChatGPT and DeepSeek were employed interchangeably, with the choice of tool depending on their relative effectiveness for each specific task. In the following I will break down the general types of prompts and how they did help or not help:
        \begin{itemize}
		\item "Copy text and ask for re-phrasing for a scientific paper and correct usage of English"
		\item "Copy code, explain the error and ask for explanation and re-write of the code"
		\item "Ask about the structure of a scientific paper"
            \item "Copy code and ask for reformat/improvements"
	\end{itemize}
        
    
    \bibliographystyle{ACM-Reference-Format}
    \bibliography{references}
    
\end{document}
